#9-oct

>[!question] 
>Let $A$ be a matrix with rank $r$ . Then there exist invertible matrices $P$ and $Q$ such that 

$$
\begin{align}

PAQ=\left[\begin{array}{@{}c|c@{}}
 I_{r} 
 & 0 \\
\hline
 0 &
0
\end{array}\right]

\end{align}
$$
Let A be a $m\times n$ let $k$ be the nullity of $A$ ie, $\dim\nsp{A}$
$A:F^{n}\to F^{m}$
let $(u_{1},\dots,u_{r})$ be a basis of $\nsp{A}$
extend this basis to a basis of $F^{n}$ say $\mathcal{B}_{1}=(v_{1},\dots v_{n-k},u_{1}\dots u_{k})$ 
$\mathcal{B}_{2}=(Av_{1}\dots Av_{n-k})$ is a basis for $\csp{A}\subseteq F^{m}$
Extend this to $\mathcal{B}_{2}=(Av_{1},Av_{2}\dots Av_{n-k},w_{1}\dots w_{m-(n-k)})$ a basis of $F^{m}$
Let $Q$ be the matrix whose columns are elements of $\mathcal{B}_{1}$ and $P$ is the inverse of the matrix whose columns are formed by elements of $\mathcal{B}_{2}$

$$
\begin{align}
PAQ(e_{j} ) & =PA(v_{j} ) \quad \ \forall \ 1\leq j\leq r \\
 & =P(Av_{j} ) \\
 & =e_{j} \text{ as } P \text{ is the inverse of } Av_{j} \\

\end{align}
$$
$$
\begin{align}
PAQ(e_{j} ) & =PAu_{i} \quad \forall r<j\leq k\\
 & =P(0) \\
 & =0 
\end{align}
$$
>[!thm] 
>Every non null matrix has a rank factorization.

$$
\begin{align}


P_{m\times m} A_{m\times n} Q_{n\times n} =\left[\begin{array}{@{}c|c@{}}
 I_{r} 
 & 0 \\
\hline
 0 &
0
\end{array}\right] \\


\end{align}
$$
$\text{Let } R=P^{-1} , S=Q^{-1}$
Let $R=\left[ \left.\left(R_{1}\right) _{m\times r}\right|R_{2}\right]$ where $R_{1}$ is of order $m\times r$
$S=\begin{bmatrix}S_{1} \\ S_{2}\end{bmatrix}$ $S_{1}$ is of order $r\times n$

> [!claim] 
> $A=R_{1}S_{1}$

$$
\begin{align}
A & =P^{-1} \left[\begin{array}{@{}c|c@{}}
 I_{r} 
 & 0 \\
\hline
 0 &
0
\end{array}\right] Q^{-1} \\
 & =R\left[\begin{array}{@{}c|c@{}}
 I_{r} 
 & 0 \\
\hline
 0 &
0
\end{array}\right] S \\
 & = \left[\left. R_{1} \right|R_{2} \right] \left[\begin{array}{@{}c|c@{}}
 I_{r} 
 & 0 \\
\hline
 0 &
0
\end{array}\right] \begin{bmatrix}S_{1} \\ S_{2} \end{bmatrix} \\
 & =R_{1}S_{1}
\end{align}
$$
 another proof 
 Let A be $m\times n$ of rank $r$ 
 Let $(B_{1}\dots B_{r})$ be a basis $\csp{A}$ where each $B_{i}\in F^{m}$
 So, every column of A say $A_{j} \ \ j=1,2\dots n$ is a linear combination of elements in $B_{1}\dots B_{r}$
 Let 
 $$
\begin{align}
A_{j}=B_{1}C_{1}\dots B_{r}C_{r} \quad C_{ij}\in F 
\end{align}
$$

$$
\begin{align}
	A_{j}=(B_{1}\ B_{2}\ \dots B_{r} ) _{m\times r} \begin{bmatrix}C_{1j} \\ \vdots \\ c_{rj} \end{bmatrix} _{r\times 1} 
\end{align}
$$

if $C=(c_{ij})$ then $A=BC$


> [!theorem] 
> If $(P,Q)$ is a rank factorization of A, then $(PT,T^{-1}Q)$ is also a rank factorization of A for $T$ a non-singular matrix.
> Also, every rank factorization of $A$ is of this form 

#10-oct

> [!exercise] 
> Determine all projectors of order $2\times2$ over $\RR$ 

there are 3 kinds of projection

a) $A=I_2$ 
b) $A=o$ 
c) restriction of det and trace trace=1 and det =0


> [!exercise] 
> If $A^{2}=A$ then show rank of A is the trace of A

> [!definition] 
> Given any matrix $A_{m\times n}$  there exist a matrix $G$ (generalised inverse denoted by g-inv ). Such that whenever $AX=B$ is a consistent system, ie $B\in \csp{A}$
> $GB$ is a solution and $G$ dosen't depend on $B$ 

> [!theorem] 
> Every matrix has a g-inv 


Suppose A be a $m\times n$ matrix
if $A=0$ as the $\csp{A}$ is trivial only consistent system is $AX=0$ so the $O_{n\times m}$ matrix suffices. Infact any $n\times m$ matrix works as g-inv let $A\neq0$ and as $r(A)\geq1$ therexist a rank factorization $(P,Q)$  let $C$ be a left inverse of $P$ and $D$ be right inverse of $Q$  and let $G=DC$ 

Let $AX=b$ be a consistent solution 

> [!claim] 
> $Cb$ is solution of $AGb=b$ 

Since $b\in \csp{A} \exists \ y \st Ay=b$ 

$$
\begin{align}
	A_{m\times n}  & =P_{m\times r} Q_{r\times n}  \\
		 Q_{r\times n} D_{n\times r}  & =I_{r}  \\
	C_{r\times m} P_{m\times r}  & =I_{r} 
\end{align}
$$
$$
\begin{align}
AGb=(PQ)(DC)(Ay)=P(Q)(CP)QY
=PQy=Ay=b\end{align}
$$

> [!theorem] 
> For any 2 matrices A and $G$ TFAE
>i) $G$ is a g-inv of A
>ii) AGA=A
>iii)AG is idempotent and $r(AG)=r(A)$
>iV) GA is idempotent and $r(GA)=r(A)$

$i\implies ii$
As $AX=A_{i}$ is a consistent system for columns $A_{1},\dots A_{n}$ of A $X=GA_{i}$
so $AGA_{i}=A_{i}\forall 1\leq i\leq n\implies AGA=A$ 

$ii\implies i$

Let $AX=b$ be a consistent system ie, therexist a $y$ st. $Ay=b$

$AGb=AGAy=Ay=b$

$ii\implies iii$
$AGAG=AG$ 

$r(AG)\leq r(A)=r(AGA)\leq r(AG)\implies r(AG)=r(A)$ 

Lets prove some results 
> [!theorem] 
> THAE
> i) $r(AB)=r(A)$
> ii) $\csp{AB}=\csp{A}$
> iii) $A=ABC$ for some matrix $C$ 

$i\implies ii$ 
$\csp{AB}\subseteq \csp{A}$ and the rank is same hence $\csp{AB}=\csp{A}$
$ii\implies iii$
as column of a is contain