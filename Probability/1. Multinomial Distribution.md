#20-sep
> [!definition ] Multinomial Distribution
$n$ independent and identical experiments. Each experiments results in one of $r$ possible outcomes, with probability  $p_{1},p_{2},p_{3}\dots p_{r}$ such that  $\sum^{r}_{i=1}p_{i}=1$
$X_{i}$ is number of results in outcome $i$ 
whenever $n_{1}+n_{2}+n_{3}\dots n_{r}=n$ 
$$
P\left(X_{1}=n_{1},X_{2}=n_{2}\dots X_{r}=n_{r}   \right) =\frac{n!}{n_{1}!n_{2}!n_{3}!\dots n_{r}!}p_{1}^{n_{1}} p_{2}^{n_{2}} \dots p_{r}^{n_{r}} 
$$
this is also written as the Multinomial$(n,p_{1},p_{2},p_{3}\dots p_{r})$ 

this is called the Multinomial$(n,p_{1},p_{2},p_{3}\dots p_{r})$ 

> [!remark] Multinomial as Binomial
>when $r=2$ Multinomial is same as Binomial

> [!remark ]
$$\begin{align}
N  & \subset \left\{ 1,2\dots r \right\} \\
\sum^{}_{i \in N}X_{i} &  = \text{Bin } \left( n,\sum^{}_{i\in N}p_{i} \right) 
\end{align}
$$



>[!def  ] Independence of Random Variable 
>Let $X$ take values be $\left\{ x_{1},x_{2},\dots \right\}$
>let $Y$ take values $\left\{ y_{1},y_{2},y_{3}\dots \right\}$
>The random variables $X$ and $Y$ are independent if 
>$\forall \  A \subset \left\{ x_{1},\dots \right\},b \subset \left\{ y_{1},y_{2}\dots \right\}$ 
$$
P(X \in A,Y \in B)=P(X\in A)P(Y\in B)
$$




>[!remark ] 
>Equivalently $X,Y$ are independent if for all $A \subset \mathbb{R}, B \subset \mathbb{R}$
>$P(X\in A,Y\in B)=P(X\in A)P(Y\in B)$ 



>[!Theorem ] 
>$X$ and $Y$ are independent if and only if 
>$$P(x,y)=P_{x}(x)P_{y}(y)$$

`\begin{proof}`

let $A=\left\{ x \right\} , B=\left\{ y \right\}$
$$
\begin{align}

P(X\in A,Y\in B)=P(X=x,Y=y) \\
P(X\in A)P(Y\in B)=P(X=x,Y=y)  \\

P_{}(X=x)P_{}(Y=y)=P(X=x,Y=y)\\
P_{x}(x)P_{y}(y)=P(X=x,Y=y) \\
\end{align}
$$

The other way 
$$
\begin{align}
P(X\in A,Y\in B) & =\sum_{x \in  B} \sum_{y\in A} P(X=x,Y=y) \\
 & =\sum_{x \in  A} \sum_{y\in B} p(x,y)  \\
 & =\sum_{x \in  A} \sum_{y\in B}P_{x}(x)P_{y}(y) \\
& = \sum_{x \in  A}P_{x}(x)\sum_{y\in B}P_{y}(y)  \\
& = P(X\in A)P(Y\in B)\\
\end{align}
$$


`\end{proof}`


## Example
>[!Exercise] 
>Given random number of balls of [[Poisson]] ($\lambda$) 
Each ball is colored white with probability $p$ and Black with probability $1-p$
let
>$N:$ Total number of balls
$X:$ number of white balls
$Y:$ number of black balls


Solution:
$$
\begin{align}
P(X=k) & =\sum_{n}P(X=k|N=m)P(N=m) \\
	 & =\sum_{n\geq k} \binom{n }{k} p^{k}(1-p)^{n-k}\frac{e^{ -\lambda }\lambda^{n}}{n!} \\
 & =\frac{p^{k}\lambda^{k}e^{ -\lambda }}{k!} \sum_{n\geq k} \frac{\lambda^{n-k}}{(n-k)!} \\
 & =\frac{p^{k}\lambda^{k}e^{ -\lambda }}{k!} e^{ \lambda(1-p) } \\
 & =\frac{e^{ -\lambda p }(\lambda p)^{k}}{k!} \\
 \implies P(Y=k) & = \frac{e^{ -\lambda(1-p) }[\lambda(1-p)]^{k}}{k!} \ \ \text{from symmetry }
\end{align} 
$$

^716f78

> [!remark] 
> If number of balls fixed say $n$ then clearly $X,Y=n-X$ is clearly not independent

> [!remark] 
> if $N=\text{Poisson}(\lambda)$ Then $X,Y=N-X$ are independent from[[1. Multinomial Distribution#^716f78]]
> 

> [!exercise]  
> Let number of balls $N=$ Geometric$(p)$
> $P(N=n)=(1-p)^{n}p$
> Color each ball white with probability $\tilde{p}$
> Color each ball black with probability $1-\tilde{p}$
> $X:$ Number of white balls
> $Y:$ Number of black balls
> show that $P(X,Y)$ is not independent

> [!exercise] Sum of [Poisson](Poisson) Variables
> $X=\text{Poisson}(\lambda_{1})$
> $Y=\text{Poisson}(\lambda_{2})$
> show that $X+Y=\text{Poisson}(\lambda_{1}+\lambda_{2})$
> 

> [!exercise] Sum of Binomials
> $X=\text{Bin}(n,p)$
> $Y=\text{Bin}(m,p)$
> show that $X+Y=\text{Bin}(m+n,p)$

$X,Y$ have joint pmf $p(x,y)$ 
$g:\mathbb{R}^{2}\to \mathbb{R}$
$$
E\left[ g\left( X,Y \right)  \right] =\sum_{x,y} g(x,y)p(x,y)
$$
we saw earlier 
$h:\mathbb{R}\to \mathbb{R}$
$$
E\left[ h\left( x \right)  \right] =\sum_{x}h(x)p(x)
$$
> [!theorem] 
> If $X$ and $Y$ are independent then
$$
E\left[ g(X)\cdot h(Y) \right]=E\left[   g(x)\right]\cdot E\left[h(y)\right]
$$
for some $g,h:\mathbb{R}\to \mathbb{R}$


`\begin{proof}`
$$
\begin{align}
\text{LHS } & = \sum_{x,y}g(x)\times h(y)\times p(x,y) \\
& = \sum_{x,y}g(x)\times h(y)\times p_{x}(x)\times p_{y}(y)\\
& = \left(  \sum_{x}g(x)\cdot p_{x}(x)  \right)\left( \sum_{y}h(y) \cdot* p_{y}(y)\ \right) \\
& =E\left[ g(x) \right] E\left[ h(y) \right]  \\
\end{align}
$$

`\end{proof}`

